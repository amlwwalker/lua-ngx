
#user  nobody;
worker_processes  1;

#error_log  logs/error.log;
#error_log  logs/error.log  notice;
#error_log  logs/error.log  info;

#pid        logs/nginx.pid;


events {
    worker_connections  1024;
}


http {
    include       mime.types;
    default_type  application/octet-stream;

    #log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
    #                  '$status $body_bytes_sent "$http_referer" '
    #                  '"$http_user_agent" "$http_x_forwarded_for"';

    #access_log  logs/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    #keepalive_timeout  0;
    keepalive_timeout  65;

    gzip  on;

    server {
	#this is necessary to use HTTPS because Natasha and I figured out that our current signal collection
	#has some dependency on the Chrome "runtime" object, and the runtime object is not present on http connections
	#only https. So detection as bot was sometimes happening. I believe she has made an adjustment to this detection rule
	#going forward
        listen       443 ssl;
        server_name  devbox.local;
	ssl_certificate /etc/ssl/certs/nginx-selfsigned.crt;
	ssl_certificate_key /etc/ssl/private/nginx-selfsigned.key;
	#this tells my nginx proxy to accept the larger headers that I am sending. I didn't really dig to specifically into
	#max independent cookie size and max total cookie size. I just chunked and set the allowed cookie size to something
	#big. It may be the case that you don't even need to chunk. My thought is that the chunking is to avoid any browser
	#specific cookie size limitiations, and this code below is to avoid any server side default total cookie size limitations
	client_header_buffer_size 8k;
	large_client_header_buffers 8 64k;

	#location from which test.js is served on the nginx box
	location /scripts/ {
	  root	html;
	  index	index.html index.htm;
	}
	
	#location from which all other proxied resources are served - targetted root of page for example sake, location value
	#might have to change to accomidate more dynamic use of protecting specific resources
        location / {
            default_type text/html;
	    #because sub_filter is inserting the <script> tag via a string replacement, I have to tell the downstream web server
	    #not to send me gzipped content, because if it does, I won't find the string value for <head> in it
            proxy_set_header Accept-Encoding "";
	    #this calls our lua plugin, use of access_by_lua because we want to determine if to allow access, and have full
	    #access to lua functions to make Vesper API call.
            access_by_lua_file '/usr/local/openresty/lualib/ngx/whiteops.lua';
	    #sub filter plugin must be enabled on nginx, easy command to execute from command line, this inserts our script into
	    #whatever resource dynamically
	    sub_filter '<head>' '<head><script src="https://s.gihwyz.com/static/gs/5.0.0/pagespeed.js?psv=5.0.0&spa=1&pd=acc&mo=2&ap=testapp&ci=678812&dt=6788121608586620837000"></script><script src="/scripts/test.js"></script>';
            #could likely be on, not sure just kinda copied this from someone elses example, 
            #likely think on would mean replace first occurence. So below value should likely be on.
	    sub_filter_once off;
	    #this is us reaching the proxy pass to the web application behind the proxy, only reached if Vesper said ALLOW, otherwise
	    #in this example, customer would go into gate loop, alternative to gate loop might be a 302 redirect or some other action
	    #or keep track of how many times gate was hit in a cookie, then after X occurences do a 302 to a message - easy to do
            proxy_pass http://127.0.0.1:8080;
        }

        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }

    }

}
